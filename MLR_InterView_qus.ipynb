{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3831c189-aa4d-4bd3-9366-b45d422796d9",
   "metadata": {},
   "source": [
    "## Interview Questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a1d5770-0dec-4f52-88ca-c54c57e0fa2d",
   "metadata": {},
   "source": [
    "1.) Normalization vs. Standardization:\n",
    "Normalization scales the data to a fixed range (typically 0 to 1) and is useful when features have different units. Itâ€™s often used in models that rely on distance calculations.\n",
    "Standardization rescales data to have a mean of 0 and a standard deviation of 1. This is beneficial when features vary widely and helps in models like linear regression or logistic regression.\n",
    "Both techniques can improve model performance by ensuring that no single feature dominates due to its scale.\n",
    "\n",
    "2.) Techniques to Address Multicollinearity:\n",
    "Remove highly correlated predictors: Identify and drop one of the correlated variables.\n",
    "\\Principal Component Analysis (PCA): Reduce the dimensionality of the data while preserving variance.\n",
    "Regularization techniques: Use Lasso (L1) or Ridge (L2) regression to penalize large coefficients and reduce multicollinearity.\n",
    "Variance Inflation Factor (VIF): Calculate VIF for predictors; remove predictors with high VIF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ddc5f-317d-49de-808e-9cd7b8fb59e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
