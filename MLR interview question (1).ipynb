{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917266fc-4387-49c2-a296-e0ae305733ba",
   "metadata": {},
   "source": [
    "### What is Normalization & Standardization and how is it helpful?\n",
    "### 1. **Normalization**\n",
    "Normalization rescales data to fit within a specific range, usually between 0 and 1. It’s commonly used when the data doesn’t follow a Gaussian (normal) distribution or when the algorithm relies on a defined data range.\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "X_{\\text{normalized}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "\\]\n",
    "\n",
    "- **Purpose**: Useful when features have different scales or ranges (e.g., age and income).\n",
    "- **Applications**: Useful in algorithms like neural networks and k-nearest neighbors (KNN), which use distance-based calculations and can benefit from all features being on the same scale.\n",
    "\n",
    "### 2. **Standardization**\n",
    "Standardization transforms the data so that it has a mean of 0 and a standard deviation of 1. It centers the data by subtracting the mean and then scales it by dividing by the standard deviation.\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "\\]\n",
    "where \\(\\mu\\) is the mean, and \\(\\sigma\\) is the standard deviation.\n",
    "\n",
    "- **Purpose**: Useful for data that follows a Gaussian distribution or when the algorithm assumes data is normally distributed.\n",
    "- **Applications**: Commonly used in linear regression, logistic regression, and support vector machines (SVMs), which assume normally distributed data.\n",
    "\n",
    "### Why These Techniques Are Helpful\n",
    "1. **Improved Model Performance**: Many algorithms are sensitive to the scale of data. Properly scaled data can lead to faster training times and more accurate models.\n",
    "2. **Prevents Bias in Distance-Based Algorithms**: Normalizing or standardizing ensures that features with larger ranges don't dominate distance calculations in algorithms like KNN and k-means clustering.\n",
    "3. **Stability and Convergence**: Optimization algorithms, like gradient descent, converge faster when data is on a consistent scale. This can make training more efficient and less prone to numerical instability.\n",
    "\n",
    "In summary, both techniques aim to bring data to a comparable scale, making it easier for models to learn patterns effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e50f8-c16f-49da-8339-a682695db533",
   "metadata": {},
   "source": [
    "### What techniques can be used to address multicollinearity in multiple linear regression?\n",
    "\n",
    "### 1. **Remove Highly Correlated Predictors**\n",
    "   - **Approach**: Identify and remove predictors with high correlation. \n",
    "   - **Method**: Calculate the correlation matrix of predictors and remove those with high correlation (typically, above 0.8 or 0.9).\n",
    "   - **Trade-Off**: This can simplify the model but may lose potentially valuable information.\n",
    "\n",
    "### 2. **Principal Component Analysis (PCA)**\n",
    "   - **Approach**: PCA transforms correlated predictors into a set of linearly uncorrelated components.\n",
    "   - **Method**: Create new predictor variables (principal components) that are combinations of the original variables, capturing the majority of variance in the data.\n",
    "   - **Trade-Off**: PCA can reduce multicollinearity but may make interpretation harder, as new components don’t correspond directly to original variables.\n",
    "\n",
    "### 3. **Partial Least Squares Regression (PLS)**\n",
    "   - **Approach**: Similar to PCA, PLS reduces the number of predictors to a smaller set of uncorrelated components, while also considering the response variable.\n",
    "   - **Method**: PLS creates components by maximizing the covariance between predictors and the target variable.\n",
    "   - **Trade-Off**: It handles multicollinearity and maintains some interpretability, though it still alters the original predictor structure.\n",
    "\n",
    "### 4. **Regularization Techniques (Ridge and Lasso Regression)**\n",
    "   - **Approach**: Regularization adds a penalty to the regression model to reduce the effect of correlated predictors.\n",
    "   - **Ridge Regression**: Adds an L2 penalty (squared magnitude of coefficients) to reduce the impact of multicollinearity by shrinking coefficients of correlated predictors.\n",
    "   - **Lasso Regression**: Adds an L1 penalty (absolute value of coefficients), which can shrink some coefficients to zero, effectively selecting a subset of predictors.\n",
    "   - **Trade-Off**: Regularization reduces multicollinearity, but the model becomes slightly more complex and less interpretable, especially in Ridge regression.\n",
    "\n",
    "### 5. **Variance Inflation Factor (VIF)**\n",
    "   - **Approach**: Calculate the VIF for each predictor to quantify the extent of multicollinearity.\n",
    "   - **Method**: A high VIF (typically >10) suggests that the predictor is highly collinear with others. Consider removing predictors with high VIF scores.\n",
    "   - **Trade-Off**: This technique helps identify problematic predictors, though it may require removing some potentially useful variables.\n",
    "\n",
    "### 6. **Centering Variables**\n",
    "   - **Approach**: Subtract the mean of each predictor variable from its values to center them around zero.\n",
    "   - **Method**: This does not eliminate multicollinearity but can reduce its impact on the interaction terms, especially if interaction terms are included in the model.\n",
    "   - **Trade-Off**: Centering maintains the information of predictors but may only partially reduce multicollinearity issues.\n",
    "\n",
    "### 7. **Data Collection or Transformation**\n",
    "   - **Approach**: Sometimes multicollinearity is reduced by increasing sample size or gathering additional data. Alternatively, log or polynomial transformations can help reduce correlations between variables.\n",
    "   - **Trade-Off**: Data collection may not be feasible, and transformations may make interpretation more challenging.\n",
    "\n",
    "### Summary Table of Techniques\n",
    "\n",
    "| Technique | Advantage | Drawback |\n",
    "|-----------|-----------|----------|\n",
    "| Remove Highly Correlated Predictors | Simplifies the model | Potential loss of information |\n",
    "| PCA | Reduces multicollinearity effectively | Reduces interpretability |\n",
    "| PLS | Maintains relevance to the target variable | Alters predictor structure |\n",
    "| Ridge/Lasso Regression | Reduces multicollinearity | Complexity in interpretation |\n",
    "| VIF Analysis | Identifies collinear predictors | May require variable removal |\n",
    "| Centering | Reduces impact on interaction terms | Only partial solution |\n",
    "| Data Collection/Transformation | May solve multicollinearity | Not always feasible |\n",
    "\n",
    "Each of these techniques offers a way to address multicollinearity, so the choice depends on the specific context, goals, and requirements of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50760c5c-a9f6-4241-ba2e-9c0d06c21555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
